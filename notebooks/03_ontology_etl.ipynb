{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ—ï¸ ì˜¨í†¨ë¡œì§€ êµ¬ì¶• ETL íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "ì‹¤ë¬´ ë°ì´í„°ë¡œ ì˜¨í†¨ë¡œì§€ë¥¼ ìë™ìœ¼ë¡œ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ ETLì´ë€?\n",
    "\n",
    "- **E**xtract (ì¶”ì¶œ): ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘\n",
    "- **T**ransform (ë³€í™˜): ë°ì´í„°ë¥¼ RAGì— ì í•©í•œ í˜•íƒœë¡œ ê°€ê³µ\n",
    "- **L**oad (ì ì¬): ë²¡í„° DBì— ì €ì¥\n",
    "\n",
    "### ì‹¤ë¬´ ì‹œë‚˜ë¦¬ì˜¤\n",
    "íšŒì‚¬ì˜ ê¸ˆìœµ ìš©ì–´ì§‘, ê·œì • ë¬¸ì„œ, FAQë¥¼ RAG ì‹œìŠ¤í…œì— ìë™ìœ¼ë¡œ ì ì¬"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from ontology_rag.core.rag_engine import RAGEngine\n",
    "from ontology_rag.embeddings.client import EmbeddingClient\n",
    "from ontology_rag.storage.vector_store import VectorStore\n",
    "\n",
    "load_dotenv()\n",
    "LLM_BASE_URL = os.getenv(\"LLM_BASE_URL\", \"http://localhost:11434\")\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"qwen2.5vl:72b\")\n",
    "\n",
    "# RAG ì—”ì§„ ì´ˆê¸°í™”\n",
    "embedder = EmbeddingClient(base_url=LLM_BASE_URL)\n",
    "store = VectorStore()\n",
    "rag = RAGEngine(\n",
    "    llm_base_url=LLM_BASE_URL,\n",
    "    llm_model=LLM_MODEL,\n",
    "    embedding_client=embedder,\n",
    "    vector_store=store,\n",
    ")\n",
    "\n",
    "print(\"âœ… RAG ì—”ì§„ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Extract: ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ì¶”ì¶œ\n",
    "\n",
    "### 1-1. JSON íŒŒì¼ì—ì„œ ìš©ì–´ì§‘ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸ˆìœµ ìš©ì–´ì§‘ ì˜ˆì‹œ ë°ì´í„°\n",
    "glossary_data = {\n",
    "    \"terms\": [\n",
    "        {\n",
    "            \"term\": \"ì˜¨íˆ¬ê¸ˆìœµ\",\n",
    "            \"full_name\": \"ì˜¨ë¼ì¸íˆ¬ìì—°ê³„ê¸ˆìœµ\",\n",
    "            \"definition\": \"ì˜¨ë¼ì¸ í”Œë«í¼ì„ í†µí•´ íˆ¬ììì™€ ëŒ€ì¶œìë¥¼ ì§ì ‘ ì—°ê²°í•˜ëŠ” ê¸ˆìœµ ì„œë¹„ìŠ¤\",\n",
    "            \"category\": \"ê¸ˆìœµìƒí’ˆ\"\n",
    "        },\n",
    "        {\n",
    "            \"term\": \"P2P ëŒ€ì¶œ\",\n",
    "            \"full_name\": \"Peer-to-Peer Lending\",\n",
    "            \"definition\": \"ê°œì¸ ê°„ ì§ì ‘ ëŒ€ì¶œì„ ì¤‘ê°œí•˜ëŠ” ì„œë¹„ìŠ¤ë¡œ, ì˜¨íˆ¬ê¸ˆìœµì˜ ì´ì „ ëª…ì¹­\",\n",
    "            \"category\": \"ê¸ˆìœµìƒí’ˆ\"\n",
    "        },\n",
    "        {\n",
    "            \"term\": \"ì‹ ìš©ë“±ê¸‰\",\n",
    "            \"full_name\": \"Credit Rating\",\n",
    "            \"definition\": \"ê°œì¸ì´ë‚˜ ê¸°ì—…ì˜ ì‹ ìš©ë„ë¥¼ í‰ê°€í•œ ë“±ê¸‰ (1~10ë“±ê¸‰)\",\n",
    "            \"category\": \"ì‹ ìš©í‰ê°€\"\n",
    "        },\n",
    "        {\n",
    "            \"term\": \"ì—°ì²´ìœ¨\",\n",
    "            \"full_name\": \"Delinquency Rate\",\n",
    "            \"definition\": \"ëŒ€ì¶œê¸ˆì„ ì•½ì •ì¼ì— ìƒí™˜í•˜ì§€ ëª»í•œ ë¹„ìœ¨\",\n",
    "            \"category\": \"ë¦¬ìŠ¤í¬ê´€ë¦¬\"\n",
    "        },\n",
    "        {\n",
    "            \"term\": \"ì¤‘ê¸ˆë¦¬\",\n",
    "            \"full_name\": \"Mid-rate Interest\",\n",
    "            \"definition\": \"ì—° 10~20% ìˆ˜ì¤€ì˜ ê¸ˆë¦¬ë¡œ, ì €ê¸ˆë¦¬ì™€ ê³ ê¸ˆë¦¬ì˜ ì¤‘ê°„\",\n",
    "            \"category\": \"ê¸ˆë¦¬\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "data_dir = Path(\"../data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "with open(data_dir / \"glossary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(glossary_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… ìš©ì–´ì§‘ ì €ì¥: {len(glossary_data['terms'])}ê°œ ìš©ì–´\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. CSV í˜•íƒœì˜ FAQ ë°ì´í„°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAQ ë°ì´í„°\n",
    "faq_data = \"\"\"ì§ˆë¬¸,ë‹µë³€,ì¹´í…Œê³ ë¦¬\n",
    "\"ìµœì†Œ íˆ¬ìê¸ˆì•¡ì€ ì–¼ë§ˆì¸ê°€ìš”?\",\"í¬í”Œì˜ ìµœì†Œ íˆ¬ìê¸ˆì•¡ì€ 10ë§Œì›ì…ë‹ˆë‹¤.\",\"íˆ¬ì\"\n",
    "\"íˆ¬ì ìˆ˜ìµì€ ì–¸ì œ ë°›ë‚˜ìš”?\",\"ë§¤ì›” ì›ë¦¬ê¸ˆì´ ìƒí™˜ë˜ë©°, íˆ¬ìì ê³„ì¢Œë¡œ ìë™ ì…ê¸ˆë©ë‹ˆë‹¤.\",\"íˆ¬ì\"\n",
    "\"ëŒ€ì¶œ í•œë„ëŠ” ì–´ë–»ê²Œ ê²°ì •ë˜ë‚˜ìš”?\",\"AI ì‹ ìš©í‰ê°€ ì‹œìŠ¤í…œìœ¼ë¡œ ê°œì¸ë³„ ì‹ ìš©ë„ë¥¼ ë¶„ì„í•˜ì—¬ ìµœëŒ€ 3,000ë§Œì›ê¹Œì§€ ê²°ì •ë©ë‹ˆë‹¤.\",\"ëŒ€ì¶œ\"\n",
    "\"ì¤‘ë„ ìƒí™˜ ìˆ˜ìˆ˜ë£Œê°€ ìˆë‚˜ìš”?\",\"í¬í”Œ ëŒ€ì¶œì€ ì¤‘ë„ìƒí™˜ ìˆ˜ìˆ˜ë£Œê°€ ì—†ìŠµë‹ˆë‹¤.\",\"ëŒ€ì¶œ\"\n",
    "\"íˆ¬ì ì›ê¸ˆ ì†ì‹¤ ê°€ëŠ¥ì„±ì´ ìˆë‚˜ìš”?\",\"ëŒ€ì¶œìì˜ ì—°ì²´ë‚˜ ë¶€ë„ ì‹œ ì›ê¸ˆ ì†ì‹¤ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë©°, ì´ëŠ” íˆ¬ì ìœ„í—˜ì— í•´ë‹¹í•©ë‹ˆë‹¤.\",\"ë¦¬ìŠ¤í¬\"\n",
    "\"\"\"\n",
    "\n",
    "with open(data_dir / \"faq.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(faq_data)\n",
    "\n",
    "print(\"âœ… FAQ ë°ì´í„° ì €ì¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Transform: ë°ì´í„° ë³€í™˜ ë° êµ¬ì¡°í™”\n",
    "\n",
    "### 2-1. JSON ìš©ì–´ì§‘ â†’ ë¬¸ì„œ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_glossary(json_path):\n",
    "    \"\"\"ìš©ì–´ì§‘ JSONì„ RAG ë¬¸ì„œë¡œ ë³€í™˜\"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    documents = []\n",
    "    for term_info in data[\"terms\"]:\n",
    "        # êµ¬ì¡°í™”ëœ ë¬¸ì„œ ìƒì„±\n",
    "        doc = f\"\"\"[ìš©ì–´] {term_info['term']}\n",
    "[ì˜ë¬¸/ì „ì²´ëª…] {term_info['full_name']}\n",
    "[ì •ì˜] {term_info['definition']}\n",
    "[ë¶„ë¥˜] {term_info['category']}\"\"\"\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "glossary_docs = transform_glossary(data_dir / \"glossary.json\")\n",
    "\n",
    "print(f\"âœ… ìš©ì–´ì§‘ ë³€í™˜ ì™„ë£Œ: {len(glossary_docs)}ê°œ ë¬¸ì„œ\")\n",
    "print(f\"\\nì˜ˆì‹œ:\\n{glossary_docs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. CSV FAQ â†’ ë¬¸ì„œ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def transform_faq(csv_path):\n",
    "    \"\"\"FAQ CSVë¥¼ RAG ë¬¸ì„œë¡œ ë³€í™˜\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            doc = f\"\"\"[FAQ - {row['ì¹´í…Œê³ ë¦¬']}]\n",
    "ì§ˆë¬¸: {row['ì§ˆë¬¸']}\n",
    "ë‹µë³€: {row['ë‹µë³€']}\"\"\"\n",
    "            documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "faq_docs = transform_faq(data_dir / \"faq.csv\")\n",
    "\n",
    "print(f\"âœ… FAQ ë³€í™˜ ì™„ë£Œ: {len(faq_docs)}ê°œ ë¬¸ì„œ\")\n",
    "print(f\"\\nì˜ˆì‹œ:\\n{faq_docs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì„ íƒì‚¬í•­)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì •ì œ\"\"\"\n",
    "    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°\n",
    "    text = \" \".join(text.split())\n",
    "    # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•„ìš”ì‹œ)\n",
    "    # text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s\\[\\]\\-:,.]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# ëª¨ë“  ë¬¸ì„œ ì „ì²˜ë¦¬\n",
    "all_docs = glossary_docs + faq_docs\n",
    "processed_docs = [preprocess_text(doc) for doc in all_docs]\n",
    "\n",
    "print(f\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: ì´ {len(processed_docs)}ê°œ ë¬¸ì„œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Load: ë²¡í„° DBì— ì ì¬\n",
    "\n",
    "### 3-1. ê¸°ì¡´ ë°ì´í„° ì´ˆê¸°í™” (ì„ íƒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¹¨ë—í•˜ê²Œ ì‹œì‘í•˜ë ¤ë©´ ì£¼ì„ í•´ì œ\n",
    "# store.clear()\n",
    "# print(\"ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ\")\n",
    "\n",
    "print(f\"í˜„ì¬ ì €ì¥ëœ ë¬¸ì„œ: {store.count()}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. ë°°ì¹˜ ì ì¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ ì ì¬\n",
    "rag.add_documents(processed_docs)\n",
    "\n",
    "print(f\"âœ… ì ì¬ ì™„ë£Œ!\")\n",
    "print(f\"ì´ ë¬¸ì„œ ìˆ˜: {store.count()}ê°œ\")\n",
    "print(f\"- ìš©ì–´ì§‘: {len(glossary_docs)}ê°œ\")\n",
    "print(f\"- FAQ: {len(faq_docs)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ ê²€ì¦: êµ¬ì¶•ëœ ì˜¨í†¨ë¡œì§€ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "### 4-1. ìš©ì–´ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"ì˜¨íˆ¬ê¸ˆìœµì´ ë­ì•¼?\",\n",
    "    \"ì¤‘ê¸ˆë¦¬ê°€ ë­”ê°€ìš”?\",\n",
    "    \"P2P ëŒ€ì¶œ ì„¤ëª…í•´ì¤˜\",\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª ìš©ì–´ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\\n\" + \"=\"*80)\n",
    "\n",
    "for q in test_questions:\n",
    "    answer = rag.query(q, top_k=2)\n",
    "    print(f\"\\nì§ˆë¬¸: {q}\")\n",
    "    print(f\"ë‹µë³€: {answer}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. FAQ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_questions = [\n",
    "    \"ìµœì†Œ íˆ¬ìê¸ˆì•¡ ì•Œë ¤ì¤˜\",\n",
    "    \"ëŒ€ì¶œ í•œë„ëŠ” ì–¼ë§ˆì•¼?\",\n",
    "    \"ì¤‘ë„ ìƒí™˜ ìˆ˜ìˆ˜ë£Œ ìˆì–´?\",\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª FAQ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\\n\" + \"=\"*80)\n",
    "\n",
    "for q in faq_questions:\n",
    "    answer = rag.query(q, top_k=2)\n",
    "    print(f\"\\nì§ˆë¬¸: {q}\")\n",
    "    print(f\"ë‹µë³€: {answer}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ ê³ ê¸‰: ìë™ ì—…ë°ì´íŠ¸ íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "### 5-1. ì¦ë¶„ ì—…ë°ì´íŠ¸ (ìƒˆ ë°ì´í„°ë§Œ ì¶”ê°€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_update(new_data_path):\n",
    "    \"\"\"ìƒˆë¡œìš´ ë°ì´í„°ë§Œ ì¶”ê°€\"\"\"\n",
    "    # 1. Extract\n",
    "    with open(new_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        new_data = json.load(f)\n",
    "    \n",
    "    # 2. Transform\n",
    "    new_docs = transform_glossary(new_data_path)\n",
    "    \n",
    "    # 3. Load\n",
    "    rag.add_documents(new_docs)\n",
    "    \n",
    "    return len(new_docs)\n",
    "\n",
    "# ì˜ˆì‹œ: ìƒˆ ìš©ì–´ ì¶”ê°€\n",
    "new_terms = {\n",
    "    \"terms\": [\n",
    "        {\n",
    "            \"term\": \"ì—ì–´íŒ©\",\n",
    "            \"full_name\": \"AIRPACK\",\n",
    "            \"definition\": \"PFCTì˜ AI ê¸°ë°˜ ì‹ ìš© ë¦¬ìŠ¤í¬ ê´€ë¦¬ B2B ì†”ë£¨ì…˜\",\n",
    "            \"category\": \"ì œí’ˆ\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(data_dir / \"new_terms.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(new_terms, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "added = incremental_update(data_dir / \"new_terms.json\")\n",
    "print(f\"âœ… {added}ê°œ ìƒˆ ìš©ì–´ ì¶”ê°€\")\n",
    "print(f\"ì´ ë¬¸ì„œ ìˆ˜: {store.count()}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2. ì „ì²´ ETL íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_etl_pipeline(data_sources, clear_existing=False):\n",
    "    \"\"\"ì „ì²´ ETL íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\"\"\"\n",
    "    print(\"ğŸš€ ETL íŒŒì´í”„ë¼ì¸ ì‹œì‘\\n\")\n",
    "    \n",
    "    # 0. ì´ˆê¸°í™” (ì„ íƒ)\n",
    "    if clear_existing:\n",
    "        store.clear()\n",
    "        print(\"ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ\")\n",
    "    \n",
    "    all_documents = []\n",
    "    \n",
    "    # 1. Extract & Transform\n",
    "    for source in data_sources:\n",
    "        source_type = source[\"type\"]\n",
    "        source_path = source[\"path\"]\n",
    "        \n",
    "        print(f\"ğŸ“¥ ì²˜ë¦¬ ì¤‘: {source_path}\")\n",
    "        \n",
    "        if source_type == \"glossary\":\n",
    "            docs = transform_glossary(source_path)\n",
    "        elif source_type == \"faq\":\n",
    "            docs = transform_faq(source_path)\n",
    "        else:\n",
    "            print(f\"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì…: {source_type}\")\n",
    "            continue\n",
    "        \n",
    "        all_documents.extend(docs)\n",
    "        print(f\"  âœ“ {len(docs)}ê°œ ë¬¸ì„œ ë³€í™˜\")\n",
    "    \n",
    "    # 2. Preprocess\n",
    "    processed = [preprocess_text(doc) for doc in all_documents]\n",
    "    print(f\"\\nğŸ”§ ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed)}ê°œ ë¬¸ì„œ\")\n",
    "    \n",
    "    # 3. Load\n",
    "    rag.add_documents(processed)\n",
    "    print(f\"\\nâœ… ì ì¬ ì™„ë£Œ: ì´ {store.count()}ê°œ ë¬¸ì„œ\")\n",
    "    \n",
    "    return store.count()\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "data_sources = [\n",
    "    {\"type\": \"glossary\", \"path\": data_dir / \"glossary.json\"},\n",
    "    {\"type\": \"faq\", \"path\": data_dir / \"faq.csv\"},\n",
    "]\n",
    "\n",
    "# total = run_etl_pipeline(data_sources, clear_existing=True)\n",
    "print(\"\\nğŸ’¡ ìœ„ ì½”ë“œ ì£¼ì„ì„ í•´ì œí•˜ë©´ ì „ì²´ íŒŒì´í”„ë¼ì¸ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ ëª¨ë‹ˆí„°ë§ ë° í’ˆì§ˆ ê´€ë¦¬\n",
    "\n",
    "### 6-1. ë¬¸ì„œ í†µê³„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics():\n",
    "    \"\"\"ë¬¸ì„œ í†µê³„ í™•ì¸\"\"\"\n",
    "    results = store.get_all()\n",
    "    docs = results['documents']\n",
    "    \n",
    "    total = len(docs)\n",
    "    avg_length = sum(len(doc) for doc in docs) / total if total > 0 else 0\n",
    "    \n",
    "    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜\n",
    "    categories = {}\n",
    "    for doc in docs:\n",
    "        if \"[ìš©ì–´]\" in doc:\n",
    "            categories[\"ìš©ì–´ì§‘\"] = categories.get(\"ìš©ì–´ì§‘\", 0) + 1\n",
    "        elif \"[FAQ\" in doc:\n",
    "            categories[\"FAQ\"] = categories.get(\"FAQ\", 0) + 1\n",
    "        else:\n",
    "            categories[\"ê¸°íƒ€\"] = categories.get(\"ê¸°íƒ€\", 0) + 1\n",
    "    \n",
    "    print(\"ğŸ“Š ë¬¸ì„œ í†µê³„\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"ì´ ë¬¸ì„œ ìˆ˜: {total}ê°œ\")\n",
    "    print(f\"í‰ê·  ë¬¸ì„œ ê¸¸ì´: {avg_length:.0f}ì\")\n",
    "    print(f\"\\nì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:\")\n",
    "    for cat, count in categories.items():\n",
    "        print(f\"  - {cat}: {count}ê°œ ({count/total*100:.1f}%)\")\n",
    "\n",
    "get_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2. ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_search_quality(test_cases):\n",
    "    \"\"\"ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€\"\"\"\n",
    "    print(\"ğŸ¯ ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸\\n\" + \"=\"*80)\n",
    "    \n",
    "    for i, (question, expected_keyword) in enumerate(test_cases, 1):\n",
    "        answer = rag.query(question, top_k=2)\n",
    "        \n",
    "        # ê¸°ëŒ€ í‚¤ì›Œë“œê°€ ë‹µë³€ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸\n",
    "        is_correct = expected_keyword.lower() in answer.lower()\n",
    "        status = \"âœ…\" if is_correct else \"âŒ\"\n",
    "        \n",
    "        print(f\"\\n{i}. {status} ì§ˆë¬¸: {question}\")\n",
    "        print(f\"   ê¸°ëŒ€ í‚¤ì›Œë“œ: {expected_keyword}\")\n",
    "        print(f\"   ë‹µë³€: {answer[:100]}...\")\n",
    "        print(\"-\"*80)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\n",
    "test_cases = [\n",
    "    (\"ì˜¨íˆ¬ê¸ˆìœµì´ ë­ì•¼?\", \"ì˜¨ë¼ì¸íˆ¬ìì—°ê³„ê¸ˆìœµ\"),\n",
    "    (\"ìµœì†Œ íˆ¬ìê¸ˆì•¡ì€?\", \"10ë§Œì›\"),\n",
    "    (\"ì¤‘ê¸ˆë¦¬ ì„¤ëª…í•´ì¤˜\", \"10~20%\"),\n",
    "]\n",
    "\n",
    "test_search_quality(test_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ì •ë¦¬\n",
    "\n",
    "### ETL íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì™„ë£Œ! ğŸ‰\n",
    "\n",
    "**ë°°ìš´ ë‚´ìš©:**\n",
    "1. âœ… ë‹¤ì–‘í•œ í˜•ì‹(JSON, CSV)ì—ì„œ ë°ì´í„° ì¶”ì¶œ\n",
    "2. âœ… êµ¬ì¡°í™”ëœ ë¬¸ì„œë¡œ ë³€í™˜\n",
    "3. âœ… ë²¡í„° DBì— íš¨ìœ¨ì ìœ¼ë¡œ ì ì¬\n",
    "4. âœ… ì¦ë¶„ ì—…ë°ì´íŠ¸ êµ¬í˜„\n",
    "5. âœ… í’ˆì§ˆ ëª¨ë‹ˆí„°ë§\n",
    "\n",
    "### ì‹¤ë¬´ ì ìš© íŒ\n",
    "\n",
    "1. **ìë™í™”**: í¬ë¡ ì¡ì´ë‚˜ ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ ì£¼ê¸°ì  ì‹¤í–‰\n",
    "2. **ë²„ì „ ê´€ë¦¬**: ë¬¸ì„œ ë³€ê²½ ì´ë ¥ ì¶”ì \n",
    "3. **A/B í…ŒìŠ¤íŠ¸**: ë‹¤ë¥¸ ì²­í¬ í¬ê¸°, ì „ì²˜ë¦¬ ë°©ë²• ë¹„êµ\n",
    "4. **ëª¨ë‹ˆí„°ë§**: ê²€ìƒ‰ í’ˆì§ˆ ì§€í‘œ ì¶”ì \n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "- PDF, Word ë¬¸ì„œ ì²˜ë¦¬\n",
    "- ì›¹ í¬ë¡¤ë§ ìë™í™”\n",
    "- ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ íŒŒì´í”„ë¼ì¸\n",
    "- ë©€í‹°ëª¨ë‹¬ ë°ì´í„° (ì´ë¯¸ì§€, í‘œ) ì²˜ë¦¬"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ontology-rag)",
   "language": "python",
   "name": "ontology-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
